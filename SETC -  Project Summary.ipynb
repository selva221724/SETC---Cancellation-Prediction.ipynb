{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETC - Project Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is based on the SETC of Tamilnadu which has given a larget data set for 3 months. The data set is divided into the three parts such as Booking, Cancellation and Consol. The Booking data has 37 feature of booking such as date of booking, date of journey, PNR number, region, fare...etc. The cancellation data has the information of cancelled tickets on the basis of PNR number and ticket number.  The console data has the full details of passengers on each PNR number with features like stopping point, starting point, category...etc.  The main target variable is not provided in many of dataset where it was created from the cancellation data set. The target variable is to predict the cancellation of the passenger. The predictive model will guess the passage whether he or she is going to cancel the ticket before the date of journey. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Insights and Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the data and provide the visualisation report of features depends on one and other. \n",
    "\n",
    "\n",
    "- A trained model which can predict the passager cancellation based on factors as inputs. This will be used to prevent the last minute income loss for the bus transport. \n",
    "\n",
    "\n",
    "- Top factors which affect the passenger cancellation with the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data cleaning and wrangling is the part of the Data science project where the workflow the project go through this stage. because the damaged and missing data will lead to the disaster in the accuracy and quality of the model. If the data is already structured and cleaned, there is no need for the data cleaning. In this case, the given data is structured and labelled, but the data has multiple parameters of errors and missing values. They were treated using the data cleaning, data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Model\n",
    "The machine learning models used in this project are\n",
    "\n",
    "- Random Forest classifier\n",
    "- Logistic Regression\n",
    "\n",
    "Both machine learning algorithms are best for classification and labelled data. The train and test data are divided and fitted into the model and passed through the machine learning. Since we have already noted the severe imbalance in the values within the target variable, we implement the SMOTE method in the dealing with this skewed value via the learn Python package. The predicted data and test data achieved the accuracy rate of,\n",
    "\n",
    "- Random Forest classifier: **94%** accuracy\n",
    "- Logistic Regression: **92%** accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
